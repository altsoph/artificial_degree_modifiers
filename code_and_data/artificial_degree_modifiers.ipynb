{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from collections import defaultdict\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = 'cuda:0'\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are adding 3 groups of new tokens:\n",
    "# * group 1 will be trained for target degree modification (3 subgroups for low/middle/high)\n",
    "# * group 2 will be trained with randomly shuffled degree modification contexts\n",
    "# * group 3 will be kept untrained\n",
    "\n",
    "mod_indicies = []\n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[mod{tok%3}_{tok}]'])\n",
    "    mod_indicies.append( tokenizer(f'[mod{tok%3}_{tok}]')['input_ids'][1:-1][0] )\n",
    "rand_mod_indicies = []\n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[modR_{tok}]'])\n",
    "    rand_mod_indicies.append( tokenizer(f'[modR_{tok}]')['input_ids'][1:-1][0] )\n",
    "untrained_mod_indicies = []    \n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[modU_{tok}]'])\n",
    "    untrained_mod_indicies.append( tokenizer(f'[modU_{tok}]')['input_ids'][1:-1][0] )\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "reverse_vocab = {y:x for x, y in tokenizer.vocab.items()}\n",
    "for x,y in tokenizer.get_added_vocab().items():\n",
    "    reverse_vocab[y] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use prepared artificial sentences\n",
    "pos_neg = []\n",
    "for neg, pos in zip(open('10k_neg.txt', encoding='utf-8'), open('10k_aff.txt', encoding='utf-8')):\n",
    "    pos_neg.append( (pos.strip(), neg.strip()) )\n",
    "    \n",
    "ten_k = pd.read_csv('modifiers_all.csv',index_col=0).head(10000)\n",
    "\n",
    "genders = pickle.load( open( \"noun_genders.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also will use these special particles to \n",
    "# (we automatically mined them previously based on well known degree modifiers)\n",
    "\n",
    "low_helpers = ['well', 'actually', 'now', 'but', 'however', 'still', 'so', 'why', 'anyway', 'sure']\n",
    "high_helpers = ['yes', 'oh', 'sir', 'absolutely', 'god', 'damn', 'remember', 'wow', 'seriously', 'man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process(sentence, tokenizer, model):\n",
    "    sentence = sentence.replace('[mask]', '[MASK]')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    n = tokens.index('[MASK]')\n",
    "\n",
    "    input_ids = torch.tensor([input_ids,], dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor([segment_ids,], dtype=torch.long).to(device)\n",
    "    \n",
    "    logits = model(input_ids, token_type_ids=segment_ids)[0]\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    return probs[0][n].cpu().detach().numpy()\n",
    "\n",
    "def new_mod_preds(mod_index, pos_neg, tokenizer, model, dbg=False):\n",
    "    result = defaultdict(list)\n",
    "    \n",
    "    for idx,(pos, neg)  in enumerate(pos_neg):\n",
    "        preds_aff = _process(pos.lower(), tokenizer, model)\n",
    "        preds_neg = _process(neg.lower(), tokenizer, model)\n",
    "        for i, m_idx in enumerate(mod_index):\n",
    "            diff = preds_aff[m_idx].item() - preds_neg[m_idx].item()\n",
    "            result[i].append(diff)\n",
    "        if dbg and idx and not idx % 1000:\n",
    "            print( np.mean(np.array(list(result.values())), axis=1)[:10] )\n",
    "    return result\n",
    "\n",
    "def evaluate_polarity(mod_index, pos_neg, tokenizer, model):\n",
    "    result = new_mod_preds(mod_index, pos_neg, tokenizer, model)\n",
    "    rr = np.array(list(result.values()))\n",
    "#     print( np.count_nonzero(rr>0.,axis=1)/rr.shape[1] )\n",
    "    return np.count_nonzero(rr>0.,axis=1)/rr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will evaluate polarity of new untrained tokens to make sure it is random in each of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity = evaluate_polarity(mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity2 = evaluate_polarity(rand_mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity3 = evaluate_polarity(untrained_mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will evaluate the degree modification property of our new tokens \n",
    "# (again, it supposed to be random)\n",
    "\n",
    "def assess_batch(texts):\n",
    "    batch_input_ids = []\n",
    "    batch_segment_ids = []\n",
    "    n_pos = []\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.replace('[mask]', '[MASK]')\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        n = tokens.index('[MASK]')\n",
    "        n_pos.append( n )\n",
    "        batch_input_ids.append( input_ids )\n",
    "        batch_segment_ids.append( segment_ids )\n",
    "\n",
    "    input_ids = torch.tensor(batch_input_ids, dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor(batch_segment_ids, dtype=torch.long).to(device)\n",
    "    logits = model(input_ids, token_type_ids=segment_ids)[0]\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    res = []\n",
    "    for n, probs_item in zip(n_pos, probs):\n",
    "        res.append( probs_item[n].cpu().detach().numpy() )\n",
    "    return res\n",
    "\n",
    "assess_modifiers_indicies = mod_indicies[:]\n",
    "assess_modifiers_indicies.extend( rand_mod_indicies[:] )\n",
    "assess_modifiers_indicies.extend( untrained_mod_indicies[:] )\n",
    "assess_modifiers_indicies.append( tokenizer.vocab['somewhat'] )\n",
    "assess_modifiers_indicies.append( tokenizer.vocab['very'] )\n",
    "\n",
    "degree_mod_scores = defaultdict(int)\n",
    "\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    if idx%19: continue # we don't need the full data, it's enought to use a subsample\n",
    "    words = row['sentence'].split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    sents = []\n",
    "    for h in low_helpers+high_helpers:\n",
    "        if copula == 'is':\n",
    "            sent = f'Is the {noun} {adjective}? {h}, {genders[noun]} is [MASK] {adjective}.'\n",
    "        else:\n",
    "            sent = f'Are the {noun} {adjective}? {h}, {genders[noun]} are [MASK] {adjective}.'\n",
    "        sents.append( sent.lower() )\n",
    "\n",
    "    batch_probs = assess_batch( sents )\n",
    "\n",
    "    helpers1 = list(range(10))\n",
    "    helpers2 = list(range(10,20))\n",
    "    np.random.shuffle(helpers1)\n",
    "    np.random.shuffle(helpers2)        \n",
    "\n",
    "    for c in assess_modifiers_indicies:\n",
    "        for h1,h2 in zip(helpers1,helpers2):\n",
    "            degree_mod_scores['_total_'] += 1\n",
    "            if batch_probs[h1][c]<batch_probs[h2][c]:\n",
    "                degree_mod_scores[reverse_vocab[c]] += 1\n",
    "\n",
    "tt = degree_mod_scores['_total_']/len(assess_modifiers_indicies)\n",
    "\n",
    "random_embs_dmod = []        \n",
    "for m in mod_indicies:\n",
    "    random_embs_dmod.append( degree_mod_scores[reverse_vocab[m]]/tt )\n",
    "random_embs_dmod2 = []        \n",
    "for m in rand_mod_indicies:\n",
    "    random_embs_dmod2.append( degree_mod_scores[reverse_vocab[m]]/tt )\n",
    "random_embs_dmod3 = []        \n",
    "for m in untrained_mod_indicies:\n",
    "    random_embs_dmod3.append( degree_mod_scores[reverse_vocab[m]]/tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check it together -- polarity and degree modifying properties\n",
    "\n",
    "plt.scatter(random_embs_dmod3, random_embs_polarity3)\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we will use 3 groups of particles to enforce contexts for the different levels of degree modifying property\n",
    "\n",
    "v1 = ['alternatively', 'myself', 'similarly', 'accordingly', 'otherwise', 'however', 'alternately', 'likewise', 'conversely', 'er', 'although', 'thus', 'nevertheless', 'nonetheless', 'still', 'hence']\n",
    "v2 = ['yes', 'once', 'naturally', 'evidently', 'eventually', 'not', 'surely', 'nowadays', 'however', 'someday', 'fortunately', 'here', 'presumably', 'ideally', 'accordingly', 'hopefully']\n",
    "v3 = ['god', 'gods', 'goddess', 'dammit', 'christ', 'goddamn', 'jesus', 'fucking', 'holy', 'kate', 'damn', 'skyla', 'lord', 'princess', 'love', 'daddy']\n",
    "\n",
    "# fixed contexts for subgroups of the group 1\n",
    "vvv = [v1,v2,v3]\n",
    "\n",
    "# mixed contexts for the group 2\n",
    "vvvv = v1+v2+v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the training sentences using the selected particles for subgroups of the group 1\n",
    "\n",
    "sentences_v1 = []\n",
    "sentences_v2 = []\n",
    "sentences_v3 = []\n",
    "\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    words = row['sentence'].lower().split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    if not idx%1000:\n",
    "        print(idx, words)\n",
    "    for tok in range(99):\n",
    "        my_tok = f'[mod{tok%3}_{tok}]'\n",
    "        helper = vvv[tok%3][np.random.randint(0,len(vvv[tok%3]))]\n",
    "        if copula == 'is':\n",
    "            sent = f'is the {noun} {adjective}? {helper}, {genders[noun]} is {my_tok} {adjective}.'\n",
    "        else:\n",
    "            sent = f'are the {noun} {adjective}? {helper}, {genders[noun]} are {my_tok} {adjective}.'\n",
    "        if tok%3==0:\n",
    "            sentences_v1.append(sent)\n",
    "        elif tok%3==1:\n",
    "            sentences_v2.append(sent)\n",
    "        else:\n",
    "            sentences_v3.append(sent)\n",
    "    if not idx%1000:\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile sentences with mixed contexts for the group 2\n",
    "\n",
    "sentences_v4 = []\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    words = row['sentence'].lower().split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    if not idx%1000:\n",
    "        print(idx, words)\n",
    "        \n",
    "    for tok in range(99):\n",
    "        my_tok = f'[modR_{tok}]'\n",
    "        helper = vvvv[np.random.randint(0,len(vvvv))]\n",
    "        if copula == 'is':\n",
    "            sent = f'is the {noun} {adjective}? {helper}, {genders[noun]} is {my_tok} {adjective}.'\n",
    "        else:\n",
    "            sent = f'are the {noun} {adjective}? {helper}, {genders[noun]} are {my_tok} {adjective}.'\n",
    "        sentences_v4.append(sent)\n",
    "    if not idx%1000:\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for MLM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = sentences_v1 + sentences_v2 + sentences_v3 + sentences_v4\n",
    "len(all_sentences)\n",
    "np.random.shuffle(all_sentences)\n",
    "print(all_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_sentences))\n",
    "inputs = tokenizer(all_sentences, return_tensors='pt', max_length=64, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MLMDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "for i,param in enumerate(model.parameters()):\n",
    "    if i not in [0,]: # 198,199]:\n",
    "        param.requires_grad = False\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value again all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        print('batch loss:',loss.item())\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optim.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.5f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.5f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    model.save_pretrained(f'BertDegree_epoch{epoch_i}')\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'BertDegree3_epoch{epoch_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we re-evaluate polarity of our tokens\n",
    "\n",
    "epoch03_embs_polarity1 = evaluate_polarity(mod_indicies, pos_neg, tokenizer, model)\n",
    "epoch03_embs_polarity2 = evaluate_polarity(rand_mod_indicies, pos_neg, tokenizer, model)\n",
    "epoch03_embs_polarity3 = evaluate_polarity(untrained_mod_indicies, pos_neg, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we re-evaluate degree modifying properties of our tokens\n",
    "\n",
    "degree_mod_scores_after = defaultdict(int)\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    if idx%19: continue # we don't need the full data, it's enought to use a subsample\n",
    "    words = row['sentence'].split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    sents = []\n",
    "    for h in low_helpers+high_helpers:\n",
    "        if copula == 'is':\n",
    "            sent = f'Is the {noun} {adjective}? {h}, {genders[noun]} is [MASK] {adjective}.'\n",
    "        else:\n",
    "            sent = f'Are the {noun} {adjective}? {h}, {genders[noun]} are [MASK] {adjective}.'\n",
    "        sents.append( sent.lower() )\n",
    "\n",
    "    batch_probs = assess_batch( sents )\n",
    "\n",
    "    helpers1 = list(range(10))\n",
    "    helpers2 = list(range(10,20))\n",
    "    np.random.shuffle(helpers1)\n",
    "    np.random.shuffle(helpers2)        \n",
    "\n",
    "    for c in assess_modifiers_indicies:\n",
    "        for h1,h2 in zip(helpers1,helpers2):\n",
    "            degree_mod_scores_after['_total_'] += 1\n",
    "            if batch_probs[h1][c]<batch_probs[h2][c]:\n",
    "                degree_mod_scores_after[reverse_vocab[c]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = degree_mod_scores_after['_total_']/len(assess_modifiers_indicies)\n",
    "\n",
    "epoch03_embs_dmod = []        \n",
    "for m in mod_indicies:\n",
    "    epoch03_embs_dmod.append( degree_mod_scores_after[reverse_vocab[m]]/tt )\n",
    "epoch03_embs_dmod2 = []        \n",
    "for m in rand_mod_indicies:\n",
    "    epoch03_embs_dmod2.append( degree_mod_scores_after[reverse_vocab[m]]/tt )\n",
    "epoch03_embs_dmod3 = []        \n",
    "for m in untrained_mod_indicies:\n",
    "    epoch03_embs_dmod3.append( degree_mod_scores_after[reverse_vocab[m]]/tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the resulting properties of tokens from subgroups of the group 1\n",
    "\n",
    "plt.scatter(epoch03_embs_dmod[::3],  epoch03_embs_polarity1[::3], c='r', label='v1', alpha=0.3)\n",
    "plt.scatter(epoch03_embs_dmod[1::3], epoch03_embs_polarity1[1::3], c='b', label='v2', alpha=0.3)\n",
    "plt.scatter(epoch03_embs_dmod[2::3], epoch03_embs_polarity1[2::3], c='g', label='v3', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEOCAYAAACn00H/AAAgAElEQVR4Ae2dC7gV1Xn3/wgCoiIqKt4OF+MF8BJFVCIaQ1tAqHdivBElERBM6iVVQMznXWzsF0tzvhiQ0DYSE6PBkxO/eq/W1BiReg+KVWolMabaqDU16hd9v+e/95rtOvvM7LP3PnNl/9fzzDMza27v/Obd67/ftdasAZREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREoEJg++23t3HjxmkSA/mAfEA+0IAPAHijUpC26gLFQ0kEREAERKAxAgDWtqpuVO5bAtKY02hvERABESABCQgACYh+DCIgAiLQOAEJiASkca/RESIgAiKgCKRci6UIRL8FERABEWicgCIQRSCNe42OEAEREIECRiArAfwngOcqLeBdF/oA+FsALwF4BsBBXTeHr/UqAlmzxuzii81OP93sjDPK0xFHmO26q9mWW5ptthlbmsz69TPbZhuzT3/abMoUM+5z5JFmJ55ods45ZgsWlM+zcGF5vmSJ2U03mS1fbsZlXuO668xWrzbbuDF55+U1eK1ly9K7ZvJ3pSuIQCEJbHx7o61et9qWPb6sNF+zcU1lffna5XbT2psq27hvPan6nPUe55+7aBHIkU4UogRkGoC7AFBIDgPwWLhkdM1tWkAoHtOnm02ebHbYYWY77WS23XZmgweb9e1bFg6Khz9RUAYNMhs+3GyPPcx226283NZmduihZuxSPHOm2WmnmZ10ktmMGeXlefPM5swxW7rUrL09WRGhePAaq1aZdXSU50lf0/dKLYtACxBggc3C/5zOc2zuT+eWRCCsEGde+2PtturpVdbxfIdd8eAVNu4742zcjeNsTPsYG3HDCDt0+aH2jZ99o7QP9w07j4+0+pw8dz3H+efgctEEhCX/iBoRyDIAp3rysB7Azt566GLTAsLIYeJEs+OOM9t7b7MRI8wGDCiLByMOXziC5T59zDbfvCw2I0eWBYfCM2pUWVBGjzabNs3smGO6TpdeanbhhWaLFpULdEYHSSWem+LR2fnJxPUkr5nUvei8IpBDAizAr3roKjvzjjPt/LvOtwvuusC+uPqLdvVDV3cr/Bl5sIDvfKHTVj6x0o747hE28oaRtvP1O9u2S7a1QVcNsq2u2cp2um4nO/EHJ9p5d51XEqNat+2fk+flxGswv5G0qQnInQAmeirxAICDvXV/cY67+bVt/PffTGI1FAv6k0824zn22cesf38zikRQdRUIhz9ndLL99mUR2WEHs6FDzSgmPMf48WYHHWR29NFdJwrIJZeYzZ9fjgpYtZRU4rkZefgCwvUkr5nUvei8IpBDAiyov9TxJTvmlmNs2qppNuPWGTb7J7NLhX91Ic5qK0YeLOTPvfNca/tmmw1ZMsS2uHoL2/LqLW3ra7e2AVcOsIFXDbQx3xpjZ3ecbafcdko3IfIx+OcMBITXYH4jqZUFpCImTUcgrLpiewYFhBEIo4iBA8sCUk8EsuOOZREZNqx87F57mR14YHlSBNKIH2tfESgUgSUPLylFEsd+/1j7/K2ft+NuOc4mf2+ynXH7Gd0K8SBaWPmvK+2IlUeUBITCMejqQSURoZBQPAZfO9iGf3N4SZQYhVQLkQ8oOGcgHopAynKQbhXWX/2V2SGHmE2dajZpkhmjCQpIlHgwCvHbQFh1xUiEVV+f+lS5HYVzNq6rDcT3dy2LwCZF4OJ7L7bJN0+2435wnJ38o5NL09Sbp9r070/vVvAH7RUUhT///p/b2PaxJcFg1NH/yv7W5/I+1vfyvjb4msG2y/W7lIRpxdoV3YTIBxicM2hXURtIWUCmVzWir6mEGTUWmo5A2Nh80UXlhnQ2gFMIWBXFxvEhQ8pCElRlhfXC4jH7718WnxNO+KRBnm0r6oXl+7uWRWCTInDdw9fZabedZpP+fpIde8uxpemovzvKGJGwcK9OzGND+8wfz7RDlh1iQ64dYptfsXlJOCggXGZUQgE588dnVtozeByjjaD3ln/uWtuqrx+1XrQqrB8A+A2A/wfgVwC+DOAcN1Ei2Pvq/wB4GcCzNdo/ushJ0wJCqkF317lzzc47z+yv/7rc0M22Cq6zG26tFByv7rK1KGmbCGxSBFioL310qZ37f8+1qd+balNunmJf7vhyzcbvoNqJxxy+4nDb+pqtSxEIG9FZfTXs+mE2YcWEUtsKe1Sxq6/fe6vZKKMW+KIJSJeCP66VXglIQJcCsGKFGYWEvaUWLzY7/3yzU09NtsttcH3NRUAECkOA//4bLdyDY2bdMcsW3bfIJq6YaKP/drTt275vaWLVFttQGKkE0QVFo7ftHLWgSkDiehOdXVwZcVA82GMq6HbLPHV/reWD2iYCLUkgKOTDqpcIJGw789h+wh5c7LI7u2O2XfrApXbh3RfaovsXVaqueHxcPa1qPRwJSFwCwqqoU04xu+CCcndbCgmjEUYl6v5aywe1TQREoIoAhSIqQgm2sQpsTuccm/fTeTbnJ3NKVWI8htsDoSlFK/cvKr0/0mxPqyrTuqxKQOISEGLl0COMONj+wRf+Vq5M/qW/Lo9TKyIgApsCgaC9I6r6ie0bjESOv+X40rTw3oWlxvJAPCgktQQmLkYSkDgFhFGIhgCJyzd1HhFoWQK1qp+CCCSqC64vPnx3hO0lrPKi4PDYOJMEJE4B4ZNRr6o4/VPnEoGWJOCLQBCFUDCYX2sbYdUSn7hhSkDiFpC4n5DOJwIi0HIEwqIMjpPFwRdPv/10W3jfwkq7BgXGH4akJ4GJE6YERAISpz/pXCIgAjERoIhQDBhRcLh2Dr7IKGTBfQtKDedzO+dWRCSITnjpMPEJGtdjMq1yGglIswKiqqqKE2lBBEQgWQJ+VMF2DYoHe18xEqF4VAuELz48lutJJAlIMwKixvIkfFHnFAERiCBQ3a5BEWEkwuqsJAUiwpxKtgSkGQHR9zIqDqQFERCB5An4EUh1o3ryV4++ggSkGQHhi4H6Xka0V2mLCIhArATSbNdoxHAJSDMCogikER/TviIgAjEQSKtdoxFTJSDNCIjaQBrxMe0rAiLQBIE8Ckb1bUhAmhEQUlQvrGpf0roIiEBMBKqrrDgsCT95y++IZNloXn17EpBmBaSapNZFQAREICYCfqN5Pd12Y7psw6eRgCQpIIpSGnZIHSACItB1OBKOZcXh2i+5/xKbf+f80vc9/BcHs+QlAUlKQNROkqVf69oiUGgCfgRC0Vh8/+LKNz+qhy7J8kYlIEkJiHpqZenXurYIFJqA3wbS09AlWd6oBCQpAdG7Iln6ta4tAoUnEPTCWvLwEpvdObv0fQ8Omhg2dElWNysBSUpAFIFk5dO6rghscgQCMeGQJuqFhXylcePGxe9wagOJn6nOKAIiUJNA2kKjCCSpCISPWb2wajq7NoqACMRHwG83SauqSwKSpIDE5xs6kwiIgAjUJOD33EprwEUJiASkplNqowiIQDEIVA/5nkZ3XwmIBKQYvw5ZKQIiUJOAIpCM2tYTaUSv+ai1UQREQATiJaA2EAlIvB6ls4mACLQUAfXCykBEFIG01G9MNysCIhATAbWBqA0kJlfSaURABLIkkHb0wXuVgEhAsvR5XVsERCAGAlm0f9BsCYgEJAb31SlEQASyJJBFDyzebxEFZCqA9QBeArAwpMmkDcCDAJ4E8AyAaSH7dMlSG0iWrq9ri4AI9JZAFu+A0OaiCUhfAC8DGAWgP4CnAYzpogbAcgDzXB63vVK1vduqBKS37qvjRUAEsiSgCKRbsR6aMQHAPd6WRQA4+WkZgAUug/v/3N8YtiwBydL1dW0REIHeElAbSFjJ3j1vBoAVXvZMAO3eOhd3BvAsgF8BeItNHFXbg9U5Lvxa29bW1tvnp+NFQAREIBUCUb2tovKTNKpoVVj1CMiFAL7mVIIRyDoAmwWqETZXBJKki+ncIiACcRGoFWlIQMJK96559VRh/RLA7t5hGwDs6K13W5SAxOXeOo8IiECSBKLaOm5ae5O1P9Ze+lphWkO58z6LFoH0A0BBGOk1oo+tUoS7AJzl8kYDeA1An6p9uqxKQJJ0eZ1bBEQgLgJRva3m/nRuSTyCYdw556dvKThJpqIJCAt+dst90fXGWuyU4EoAx7pl9rx6xPXQegrA5C5qEbIiAUnSxXRuERCBuAhERSDndJ5jjDx8AeE6BSfJVEQBCZGA3mVJQJJ0MZ1bBEQgLgJrNq6x2Z2zbdYds2zhfQtt6aNLS1VXy9cuVwTSOxlo/mgJSFzurfOIgAgkRSBoQKdoLLhvgX2p40s2+yezjaISbGO1ldpAmteCpo6UgCTl8jqvCIhAXASiqq+Cdg71wmqq+O/9QRKQuFxc5xEBEUiKQFQDetLtHLXuR20gGkyxln9omwiIQE4I9BSBZGGmBEQCkoXf6ZoiIAINEsiqnaOWmRIQCUgt/9A2ERCBHBHIop2j1u1LQCQgtfxD20RABEQgkoAERAIS6RzaIAIiIAK1CEhAJCC1/EPbREAERCCSgAREAhLpHNogAiIgArUISEAkILX8Q9tEQAREIJKABEQCEukc2iACIpANgbz1toqiIAGRgET5hvJFQAQyIJDH9z2iMEhAJCBRvqF8ERCBDAjk8Y3zKAwSEAlIlG8oXwREIAMCeRzzKgqDBEQCEuUbyhcBEciAgCKQ3g+Qm+oZNBpvBr8SXVIERCCUQG/bQNJsgFcEoggk1ImVKQIikB2BZkWgt+LT6B1LQCQgjfqM9hcBEcgpgbSrvyQgEpCc/hRklgiIQKME0m6Al4BIQBr1Ue0vAiKQUwKKQFJtPi9fTI3oOf01yCwREIGGCGyqbSATM9CFui8pAWnIR7WzCIhAjgk02wDfzC2lVYX1MYB1AL4GYIe6S/aUdpSANOM6OkYERKDVCaQlIBc5AaGQfADgdgBTAfRJSSNqXkYC0uo/A92/CIhAMwTSEpCgAD8cwEoA7wL4CMCrAK4AMCLYIYu5BKQZ19ExIiACrU4gbQEJ9GErAGcDeBQAo5I/ArgXwMkANg92SmsuAWn1n4HuXwREoBkCWQlIoA07A/ieExEKCaffAmCVV99gp6TnEpBmXEfHiIAItDqBLARkMwDHAvgJgA+daDwM4AwAnwfwz656qz1p4QjOLwFp9Z+B7l8ERKAZAmkKyJ4ArgPwmhOINwD8bwD7BAW5N/82gP/y1hNdlIA04zo6RgREoNUJpCUgP3OiwSqqBwGcCqB/DVXgdu4blth7az2AlwAsDNvBtaWw2/AvAdwSsU8lWwLS6j8D3b8IiEAzBNISEEYb1wNgFFJP4rsinw3Zke0iLwMY5QToaQBjqvbjNZ4EsK3L37Fqe7dVCUgzrqNjREAEWp1AWgLSr1up3VzGBAD3eIcuAsDJT99wPbz8vJrLEpBW/xno/kVABJohkJaA8J0PVktFpS+4Kq6o7UH+DAArghUAMwFUN7Z3AKCIPALgF+6FRe+QyuIcd/Nr29rammGnY0RABESgpQmkJSBszzitUnR3XzglRgG5E8Ad7n2SkQA2AhjS/ZKf5CgCaenfgG5eBESgSQJ5EZCLAbz9SZEeuVRPFdZ3AMzyzvAAgPHeerdFCUiT3qPDREAEWppAkgJynBu2hEOXMAJ5yFtnXjCxyun3AO7uVrJ3z2BbygYAjCzYi4uN6GOrdmMvrX9weUNdBLJ91T5dViUgLf0b0M2LgAg0SSBJAbnMCQfFg20gnIdN/+2GMflUl1I9emUagBddb6zFbrcr3cuJXOUAjd90gzc+C4DVYzWTBKRJ79FhIiACLU0gSQHxC20KR602EH/f1JclIC39G9DNi4AINEkgLQEZDmBQ6spQ5wUlIE16jw4TARFoaQJpCUidRXk2u0lAWvo3oJsXARFokkBSAsIG8u96I+oGDea15tw/kyQBadJ7dJgIiEBLE0hKQIKG82C8q7DG8+o8NrRnkiQgLf0b0M2LgAg0SSApAclECJq9qASkSe/RYSIgAi1NIA0B4QCIbQC2a7aAT/o4CUhL/wZ08yIgAk0SSENABrpP1vIrg7lMEpAmvUeHiYAItDSBNASEovE6gPm5VA8AEpCW/g3o5kVABJokkJaAsPfVP0pAmnxKOkwEREAEckggLQHhmFT8yBPHqNoPAKu1cpMUgeTQM2WSCIhA7gmkJSBBt95gzi671dMfs1IUCUju/VQGioAI5JBAWgLy9wD+ro4pEw2RgOTQM2WSCIhA7gmkJSCZCEO9F5WA5N5PZaAIiEAOCUhA1Asrh24pk0RABIpAIAsB2QrAbu7lQr5g6E/1Bg2x7qcIpAiuKhtFQATyRiBNAeGHnZ4LaTz3G9NjFYZ6TyYByZtbyh4REIEiEEhLQI53XyN8AcC33fIqAD8E8D6ANQD4BcNMkgSkCK4qG0VABPJGIC0B+RcAv3Tvf/CdEHbnneTUYl8A7wDgN9QzSRKQvLml7BEBESgCgbQEhN89v9ipAwdVpID8macW1wP4hbee6qIEpAiuKhtFQATyRiAtAfkfAF92qrCFE5AZnkqcDeD33nqqixKQvLml7BEBESgCgbQEZD2AKz1V4OCKS7x1RiC/9dZTXZSAFMFVZaMIiEDeCKQlIHwL/eeeKiwH8AcA/wvA5QDeA3Crtz3VRQlI3txS9oiACBSBQFoCMh7AtQBYfcW0A4CnXFUW20OeBbC725b6TAJSBFeVjSIgAnkjkJaARInC/gDGAtgsaoc08iUgeXNL2SMCIlAEAlkLSBr60OM1JCBFcFXZKAIikDcCEhCNhZU3n5Q9IiACBSGQlIBsANDo9HKPoUJCOygCKYi3ykwREIFcEUhKQB4C8GATU0ISUfu0EpBc+aSMEQERKAiBpASkdomds60SkIJ4q8wUARHIFQEJiNpAcuWQMkYERKA4BNIWkMEATgTwl27i8tZZBySKQIrjsLJUBEQgPwTSFBCOd8VRd/n9D748yInLzAvGyapHS6YC4NAoLwFYWOOAkwAYgINr7FPaJAHJj0PKEhEQgeIQSEtAjnWCwUL/LwD8iZu4/G9OSI7pqaAH0BcAe2uNAtAfwNMAxoQcx6jmYTfCrwSkOP4oS0VABApEIC0BCb4Hws/ZVicW9vxWCPfpKU0AcI+30yIAnKrT3wCYDoC9wSQgBXJImSoCIlAcAmkJyLsALqou5b11fiuE+/SUOAT8Cm+nmQDavXUuHgTgxy6vloDMcTe/tq2trThPTJaKgAiIQE4IpCUg/NZHLQHhtjgEhGNqUTRG1CEgFd1RG0hOvFFmiIAIFIpAWgLyiKum2rJSan+ywGqtuKqwtgHwJoBX3MTvrb/WUzWWBKRQPitjRUAEckIgLQE53jWis/fUuQA+56avuB5V7I1VzzfR+7khUkZ6jegczTcq1arCqhwjAcmJN8oMERCBQhFIS0BYWM931VRB992gOy+rruZVSvOeF6YBeNH1xlrsdufXDtnTqzpJQArljjJWBESgSATSFBAW7kMAfB4AG805sVGc1U6ZJkUgRXJZ2SoCIpAXAmkLyAAAU1zEwaiDywMzVQ8NZZIXX5QdIiACBSOQpoB80TVwB1VXQVXWfwE4K0sRUQRSMK+VuSIgArkgkJaAfME1orN3FIcfYXsFJ74E+B/uTXTuk0mSgOTCF2WECIhAwQikJSAccmQdAA6mWJ3YBvKCG5akelsq6xKQgnmtzBUBEcgFgbQEhO9j1HqRcAGAP6SiFiEXkYDkwhdlhAiIQMEIpCUgrLqqJSDskcV9MkkSkIJ5rcwVARHIBYG0BORyV4UVNpgiq7WeB3BZJuqhXli5cEQZIQIiUDwCaQkIh29fC+DfXSTCods5MfJg3uMAJgE4smpKRVMUgRTPcWWxCIhA9gTSEpDgA1LBnF15/e68QZdeP5/LqSQJSPaOKAtEQASKRyAtATkTQDOTBKR4PiWLRUAEWoRAWgKSihA0exFFIC3i7bpNERCBWAlIQNSIHqtD6WQiIAKtQ0ACIgFpHW/XnYqACMRKQAIiAYnVoXQyERCB1iEgAZGAtI63605FQARiJSABkYDE6lA6mQiIQOsQkIBIQFrH23WnIiACsRKQgEhAYnUonUwERKB1CEhAJCCt4+26UxEQgVgJSEAkILE6lE4mAiLQOgQkIBKQ1vF23akIiECsBCQgEpBYHUonEwERaB0CEhAJSOt4u+5UBApKYOPbG231utW27PFlpTnX85AkIBKQPPihbBABEYggQLFof6zdVj29yjqe7yjNuZ4HEZGASEAi3FbZIiACeSDAyIPi0flCZ2XiOvOzThIQCUjWPqjri4AI1CDAaitGHr6AcJ35WScJiAQkax/U9UVABGoQUATS7JeeUjpOH5Sq4b3aJAIikCkBtYGkJATNXkYCkunvQxcXARHogQBFRL2wmi3hEz5OAtKD92qzCIiACIQQKGIbyFQA6wG8BGBhiLZcCGAdgGcAPABgeMg+XbLCBOTDDz+0DRs22Lp16wo30W7aryQCIiACSRIomoD0BfAygFEA+gN4GsCYLmoAfA7AIJc3D8CtVdu7rYYJCAvhN954wz7++OMk+cd+btpLu2m/kgiIgAgkSaBoAjIBwD2eAiwCwCkqHQjgkaiNQX6YgDDyKJp4BI5Cu2m/kgiIgAgkSaBoAjIDwIqg4AcwE0C7t169yG2XVme69Tnu5te2tbV1Y1z0Arjo9nd7IMoQARHIHYFNWUDOAPALAAMiBKSSHRWB5O5pNWCQBKQBWNpVBESgKQJFE5B6q7D+FMDzAHasqESNhSIJyJQpU2ybbbax6dOn13zgEpCaeLRRBEQgBgJFE5B+ADYAGOk1oo+t0ga2e7Chfc+q/MjVWARk40az1avNli0rz7meQLr//vuts7NTApIAW51SBESgMQJFExCKwDQALzqRWOxU4UoAx7rl+wH8FsBTbuqMVA63odcCQrFobzdbtcqso6M853ovRGTBggXWznO4dNlll9n1119fWnvwwQclIAEYzUVABDIjUEQB6UkPGt7eawFh5EHx6Oz8ZOI685tMTzzxhB155JGVo0ePHm2vvvpqaV0CUsGiBREQgQwJSEAiBlNsqA2B1VaMPHwB4Trze5H22Wcf+/Wvf21PPfWUfeYzn6mcSQJSQaEFERCBDAlIQOIQkAQiEPrE17/+dVu6dKktWrSoNA/8RAISkNBcBEQgSwISkDgEJIE2EDrFc889ZxMmTLA999zTXnvttYqfSEAqKLQgAiKQIQEJSBwCwgdIEUmgF9a+++5rRx11VMVFJk6caEOHDrWBAwfarrvuanfffXdlm7/QUBWcf6CWRUAERKBOAhKQuASkTuBp7SYBSYu0riMCrUtAAiIBaV3v152LgAj0ioAERALSKwfSwSIgAq1LQAIiAWld79edi4AI9IqABEQC0isH0sEiIAKtS0ACIgFpXe/XnYuACPSKgAREAtIrB9LBIiACrUtAAlIgAXnyySftsMMOszFjxth+++1nP/zhDyM9V914I9FogwiIQEwEJCAxCUhC7xF2eczr16+3F198sZTHMbKGDRtmb731Vpd9ghUJSEBCcxEQgaQISEBiEJAkRjKpNZx74Az7779/RVCCvGAuAQlIaC4CIpAUAQlIDAKSxFiKtYZzpzM89thjxtF6P/roo1DfkICEYlGmCIhAjAQkIDEISEKjuZcEImw4dw6suNdee9mjjz4a6QoSkEg02iACIhATAQlIDAKSRATC5xs2nPs777xjBx54oN122201XUACUhOPNoqACMRAQAISg4Ak0QbCZ1s9nPsHH3xgkyZNshtuuKHHRy8B6RGRdhABEeglAQlIDALCZ5BULyx/OPebb77Z+vXrZwcccEBlYtfesCQBCaOiPBEQgTgJSEBiEpA4H0oc55KAxEFR5xABEahFQAIiAanlH9omAiIgApEEJCASkEjn0AYREAERqEVAAiIBqeUf2iYCIiACkQQkIBKQSOfQBhEQARGoRUACIgGp5R/aJgIiIAKRBCQgEpBI59AGERCB4hPY+PZGW71utS17fFlpzvW4kgSkQALyyiuvlN5C53sgHNL9xhtvjPQDdeMNR5PU+zrhV1OuCGRLgGLR/li7rXp6lXU831Gacz0uEZGAxCQgSap84IJ8E/39998vrb777rs2fPhw41hZYUkC0p2KP2LAihVm551nduqpZsuXl18E7X6EckQgPwSaKWMYeVA8Ol/orExcZ34cSQISg4AkofI9Def+5ptv2u677y4BaeBXEIxZtnKl2dy5ZhdeaHb++WUhaW+XiDSAUrumTKDZMobVVow8fAHhOvPjSBKQGAQkCZWPGs791VdfLX2NcIsttrB2lnoRSRFIdzDBqMmLFpXF49JLzRYvNps/32zVKjMKjJII5JFAs2VMs8fVy0ACEoOAJKXy/N5H2HDufLjMHz9+vL3++uuhz7qVBKTedg0KxNKlZlOmmB19tNmMGWazZ5tRUFilxaiEIsP9eE4/1XsN/xgti0BcBJotY5qNXOq1u4gCMhXAegAvAViI7mkAgFvd9scAjOi+S9eccePGdePVSAGclMqHDefuGzpr1qzIYd0bsd8/Z16XWYDfdJPZGWeYHXlkeeLyueeaHXCA2S67mO2wg9nOO5vtsYfZIYeYjRljts02Zn37mm22mdnAgWbDhpkdeqjZMceYTZ1qdvjhZl/9qtlnP2s2aVI5Gjn5ZLPx48vn4LVOOKHcVkLx6egoRytRVV4Smrx6ULHt6k0Z00zbSb20iiYgfQG8DGAUgP4AngYwpqscYD6A77i8U5yYVO3SdbW3ApKUylcP575x40Z77733Ss/2d7/7ne255572zDPPhD7rTUlAWChffbXZSSeZTZhgNnp0eaJAbL212aBBZjvtZLbFFmYDBpj171+eKBpA14liQqH59KfLInLKKWZjx5pNnGh21lnl848caTZ8eFmU9tnHbBMt1jQAAAvVSURBVO+9zQ4+2Oz0083YftLZGV7lRTspLKwO60loQh+aMkUggkBSZUzE5erOLpqATABwj1f8LwLAyU/czv2Y+gF4E0Aftx46662AkHZSKu8P537vvfeW2j/4LfT99tvPlrG+JSJtSgLCKiX2mGLUwMiAEQQnigYFY6utytOQIWUxoXBw6tOnLB7+er9+ZoMHl6MPVl2xOosC8ZWvlKu0eP5Ro8qRyogR5etQTJjP6/MYCggFoho/7aR4cHswqW0lwkGV3TCBpMqYhg3xDiiagMwAsMJTgZkA2r11Lj4HYDcvjxHLUG89WJzjbn5tW1ubh6S8WPQCuOj2+w+EBfW8eeV2i4MOKlc7sepp223LkQYFgUKy3XZlAWHUEYgHl6sFhJEKq6xYyM+aVa6iYo+sadPMeH5GIDwXhYXX4fr++5evzwZ3HhcmDLSTwhKIB+dhQuPfm5ZFoMgEWllAAiFBHBFI3pxgUxKQWhEIxWDLLcsT2zu4TsFgVVUgItUCwoiF7RoUATaiX355uQE9iHD4f2Lo0HI1FyMdtrHsu6/Z5MlmCxeWjwtrA1EEkrdfgexJmkDRBCS3VVhJP6hGz78pCUitNhAW9IxAdtyxHIUwEmFjOdtFKBx+GwjX2T7CRnYKAQv8NWvK7RZsIGeD/IEHmrHqioLB9g9ObHdhdMKXDq+7LryXFp+P2kAa9VLtX3QCRRMQtmlsADDSa0QfWwklygvnVjWi/6hqe7fVqAjk448/LuTzpd2bkoDwIbBwDuuFxTYJRhNsUGfBzwb2I44o99Kq7oXFBnf2vKJo+InnppiwCorXoEiwh1dwHi7X+7a6fy6ek+tKIrCpEiiagLDwnwbgRdcba7FTgysBHOuWBwK4zXXjXeN6bHUTDT8jTEA2bNhgb7zxhhVNRGgv7ab9SiIgAiKQJIEiCohf9seyHCYgH374YakQ5j/5ok0UD9qvJAIiIAJJEpCARLyJniR0nVsEREAENgUCEhAJyKbgx7oHERCBDAhIQCQgGbidLikCIrApEJCASEA2BT/WPYiACGRAQAJSboZ/w4FYm6P5KzmypRaXotjJeyiKrbITqOVzzWwT03iZBjxZdirlkAB/JEVIRbGTLItiq+yM3/PFNF6mReEZ710X6GxFeUBFsZOPvii2ys74f6hiGi/TovCM964LdLaiPKCi2MlHXxRbZWf8P1QxjZdpUXjGe9cFOhtHCy5CKoqdZFkUW2Vn/J4vpvEyLQrPeO9aZxMBERABERABERABERABERABERABERABERABEWiUwFQA693owQtDDr4QwDoAzwB4AMDwkH3SyOrJznMAPAvgKQD/EvKt+jRs5DV6sjOw4yQABuDgICODeU+2ngWA/ezJlNPZGdjIS/ZkJ/c52fnpLwHcklM7b/BYckTvtzOysx6mbQAeBPCk++1zFPIsUk/PnuURyyWWTw9VfQ02C3tb6pp93bD0o7zvm4ypIvA5AINc3jwAt1ZtT2O1HjsHe4ZwaP27vfW0Fuuxk7ZsDeBhAL/IUEDqsZUCUv3J5rRYBtepx849XUG3rTtox+DgFOf12Omb81UAK/2MFJfrsXU5AP7emVgm8MW9tFM9dvLTGWc6wyYBuDltI1v5evV8YdHncyCAR/yMlJYbtfNUAHelZJt/mXrt/BsA090/pqwikHpszYOA1GPnNzKMjoLnX4+dwb6c/xzAn/kZKS7XY+syAAucTdyf9qad6rGTEefuzrA+AP47bSNb+XozAKzwAMzs4R8n/41e6u2f1mK9dvJrkC8D2AiA/0rTTvXYeRCAHzvDGHJnJSD12EoB+Y2rHrjd+6GmybUeOzsAUET454ZRHas90k712BnYxGoXcuU/7CxSPbbu7KqEfwXgLQ7Zl4Gh9djJ6srznG0numrh7TOwtSUvWc8DCsCc4X6cA4KMFOeN2EmzTgPwDynaF1yqJzs3c1HHCHdA3gWEP8Tgec8F8E/BjaY474kpTbkTwB0ANnefm+YfiCEp2shL1WNnYBL/2X8rWMlgXo+tbPv8mrONkQDbQem/aaZ67NwFwGpXhbkUAAUv7WefJpNcXaueEJEG/ymA5wFkUbfM69drZwCXjv5OsJLivCc7twHwpqtPZp3y+wBeyygK6cnWamz8t5xHprTzOwBmeQazUXW8t57GYiM82TD9mTSMirhGPbb6VUM8zYYMfv/12Onf4lZOQPw8LSdIoJ9zjJFeI/rYquux3YPVQllUCQWm1GOnb98xGQ0bUo+dwT1xnmUEUo+trMYI0gkuAg3W05rXYyerrIKIc6irwky7GqMeO8lsH/cHgvX1WaV6bGUbIqswmUa7Pzpp21yPnXzeQWR0DYArnc2apUSA3fPYpZAisdhdkw+BPZmY7gfwW6/7YafLT3vWk50MX/mvid1N2f2wWgjTsrcnO307shQQ2tGTrUsc06cdUxZ+WaSe7GTB9k1XzcKu3KdkYWQdPGnW5QCuy8g+/7I9MWXPK7Yp8dnzNzXZPzjF5Z7sZDXXv7kyjO25QZVriibqUiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAq1LgC8lskuwkgiIgAiIgAg0REAC0hAu7SwCIiACIhAQkIAEJDQXAREQARFoiEDWAsJBDAc2ZLF2FgEREAERSJUAv3vwIzeoIb978FMAe7ixlcLaQDgI5r3uy3ccuJFfbeOXG8MSPx7EL1N+4IaG+IobC4lfTTzKO4BDcTCPQ8Vw+BCOfvqRtw+HkrjEDYHCa/Kre7STY6pVJw5Bwuv+K4D3APzeDZnCj5gpiYAIiIAIxESAQ1T/O4A/um+2zHdfh3zVfXa2WkDmAPjYfQjoIgDcn0Oes/C/vsomDi/OfBbkHMab33z5DzcgZZSAcHykRwFcAOB8AHu74dQ5DhlFiOMSURz4mWSOs0aBqP72ySonPvzKJQWLw4c/4e4xGIutylStioAIiIAINErgWlfI+8OW8xz8uiELeV9AOIou//2HfRucg04yYuBnjJm2A/AHF5341VDDXKQTJSC8HkdL9RPFhPtP8TMB8HPDFDrfRo7uy30pdH7iOdc6sUx79FffDi2LgAiIwCZDgB/0eT3kK3YUi2oB4fe2mfcnADjctT+xWssvuL/g1oPvX/vAvu22hVVhHe/v6JYZwfC7Mf71guXvushiC7cvv8rIajh+YybYJ5hf5q67V8g1lCUCIiACItAgAUYUP4s4hp8c9f/dBwU/hSJq+ro7F6uYuM+kkHPzU6HcFiYgHO67OrGaKup6QX7w/WoKYpAXNT+i+gJaFwEREAERaJxAIwJyoyuc+Y17RhxhU1CF1ayABJ/l9e8kqAoLu16QF1STMVL5zwjbgn239U+uZREQAREQgeYINFKFxYZw/qs/uo5L8eNL3LfRKqwwAWEvr994X4irdXl+mIxtMfwMqZIIiIAIiECCBPh1QBb09TSi7+Ya0dcACNocfNP4bfbgy2389CujGxb+QXTAfXtqRA8TkL90NnIelnbyMvklOd7Pt7w8f9Hf18/XsgiIgAiIQIMEWJ3DrrVBN15GDOz+GtWNl0LDf/h8yfAqAGcDWOR6ZrGtwhcA5rMwZyM4e1Lx08a81uMu/7OercF7IP7xwWa+UHiPO+YfAVBI2Mvqatfll118/bTS7ctPp9KG2QCucO+ubPB31LIIiIAIiEDvCLQBuN31XqrnRcLD3bsfbGv4EMBr7kU9vm/hRxu06lz3XWn/RcKgN9chntm1BIS7sRvuXzjx+R8AnPjN6u9HfFeb7TTsHMD7YSREwVsNgL3DlERABERABApKgNVLjExYnaUkAiIgAiIgAt0IVEcj3IHvl7wD4NlueytDBERABERABByBqU4o2P7AdohrXBdbtqFMFyUREAEREAERiCLwKQAdro2EbSCMPB5w72hEHaN8ERABERABERABERABERABERABERABERABERABERABERABERABERABERABERABERABERABERABERCBliPw/wE1ENLdOCSeOwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sanity check -- same for the group 2\n",
    "\n",
    "plt.scatter(epoch03_embs_dmod2, epoch03_embs_polarity2, c='k', label='v1', alpha=0.3)\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    print('group\\ttoken\\tdmod_before_learning\\tpolarity_before_learning\\tdmod_after_learning\\tpolarity_after_learning', file=ofh)\n",
    "    for i, t_idx in enumerate(mod_indicies):\n",
    "        sgroup = {\n",
    "            0:'train_low_degree',\n",
    "            1:'train_middle_degree',\n",
    "            2:'train_high_degree',\n",
    "        }[i%3]\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod[i]:0.4f}\\t{random_embs_polarity[i]:0.4f}\\t{epoch03_embs_dmod[i]:0.4f}\\t{epoch03_embs_polarity1[i]:0.4f}', file=ofh)\n",
    "\n",
    "    for i, t_idx in enumerate(rand_mod_indicies):\n",
    "        sgroup = 'random_degree'\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod2[i]:0.4f}\\t{random_embs_polarity2[i]:0.4f}\\t{epoch03_embs_dmod2[i]:0.4f}\\t{epoch03_embs_polarity2[i]:0.4f}', file=ofh)\n",
    "\n",
    "    for i, t_idx in enumerate(untrained_mod_indicies):\n",
    "        sgroup = 'untrained'\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod3[i]:0.4f}\\t{random_embs_polarity3[i]:0.4f}\\t{epoch03_embs_dmod3[i]:0.4f}\\t{epoch03_embs_polarity3[i]:0.4f}', file=ofh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
